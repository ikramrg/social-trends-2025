{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4140,"sourceType":"datasetVersion","datasetId":2477},{"sourceId":5461626,"sourceType":"datasetVersion","datasetId":3155471}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-02T13:18:39.970429Z","iopub.execute_input":"2025-11-02T13:18:39.970688Z","iopub.status.idle":"2025-11-02T13:18:41.205567Z","shell.execute_reply.started":"2025-11-02T13:18:39.970661Z","shell.execute_reply":"2025-11-02T13:18:41.204422Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# √âtape 1.1 : Importer les biblioth√®ques n√©cessaires\n# Explication : Pandas pour data, Matplotlib/Seaborn pour viz, NLTK/spaCy pour NLP.\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom nltk.corpus import stopwords\nimport spacy\n\n# T√©l√©charger ressources NLTK et charger spaCy (anglais, adapt√© √† tweets)\nnltk.download('stopwords')\nnltk.download('punkt')\nnlp = spacy.load('en_core_web_sm')  # Mod√®le anglais\nstop_words = set(stopwords.words('english'))\nprint(\"Setup des libs termin√© !\")\n\n# √âtape 1.2 : Charger les deux datasets depuis Kaggle\n# Explication : Sentiment140 et Generative AI Tweets, avec v√©rification colonnes.\nsentiment140_path = '/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv'\ngenai_path = '/kaggle/input/generative-ai-tweets/GenerativeAI tweets.csv'  # Ajustez chemin exact\n\n# Charger Sentiment140\ncolumn_names = ['target', 'id', 'date', 'flag', 'user', 'text']\ndf_sent140 = pd.read_csv(sentiment140_path, encoding='ISO-8859-1', names=column_names)\nprint(\"Colonnes Sentiment140 :\", df_sent140.columns.tolist())\n\n# Charger Generative AI Tweets\ndf_genai = pd.read_csv(genai_path)\nprint(\"Colonnes Generative AI Tweets :\", df_genai.columns.tolist())\n\n# Harmoniser colonnes : Renommer si n√©cessaire\nif 'Text' in df_genai.columns:  # Corrig√© : 'Text' au lieu de 'text'\n    df_genai = df_genai.rename(columns={'Text': 'text'})  # Unifier nom\ndf_genai['target'] = None  # Ajouter colonne target vide (√† labelliser plus tard si besoin)\n\n# S√©lectionner colonnes communes pour concat√©nation\ndf_sent140_subset = df_sent140[['text', 'target']].copy()  # Copie pour √©viter warnings\ndf_genai_subset = df_genai[['text', 'target']].copy()  # Copie pour Generative AI\ndf = pd.concat([df_sent140_subset, df_genai_subset], ignore_index=True)\nprint(\"Datasets combin√©s ! Taille totale :\", len(df), \"tweets.\")\n\n# √âtape 1.3 : Gestion √âthique\n# Explication : Anonymiser, noter biais.\ndf = df.drop(columns=['user'], errors='ignore')  # Supprimer usernames si pr√©sents\nprint(\"√âthique : Usernames supprim√©s pour privacy.\")\nprint(\"Biais potentiels : Sentiment140 (2009, biais anglais/USA), Generative AI (hype IA, petit volume, pas de target).\")\nprint(\"V√©rifiez licences Kaggle (CC-BY) dans README.\")\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T13:21:03.463735Z","iopub.execute_input":"2025-11-02T13:21:03.464074Z","iopub.status.idle":"2025-11-02T13:22:26.406337Z","shell.execute_reply.started":"2025-11-02T13:21:03.464045Z","shell.execute_reply":"2025-11-02T13:22:26.405157Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# √âtape 2.1 : Analyse Statistique Basique\n# Explication : Calculer longueur tweets, distribution sentiments, outliers.\ndf['length'] = df['text'].apply(len)\nprint(\"Stats longueur tweets :\", df['length'].describe())\noutliers = df[(df['length'] < 10) | (df['length'] > 280)]  # Limite Twitter 280\nprint(f\"Outliers (trop courts/longs) : {len(outliers)}\")\n\n# Distribution sentiments (uniquement Sentiment140 a target)\nprint(\"Distribution sentiments (Sentiment140 uniquement) :\", \n      df.dropna(subset=['target'])['target'].value_counts(normalize=True))\n\n# √âtape 2.2 : Visualisations\n# Explication : Histogramme longueurs, barplot sentiments, word cloud pour th√®mes.\nplt.figure(figsize=(10, 5))\nsns.histplot(df['length'], bins=50)\nplt.title('Distribution Longueur Tweets')\nplt.show()\n\nplt.figure(figsize=(6, 4))\nsns.countplot(x='target', data=df.dropna(subset=['target']))  # Seulement lignes avec target\nplt.title('Distribution Sentiments (0=N√©gatif, 4=Positif)')\nplt.show()\n\n# Ajout de l'import manquant pour WordCloud\nfrom wordcloud import WordCloud\n\n# Word Cloud pour mots fr√©quents (√©chantillon pour √©viter surcharge m√©moire)\nsample_size = 10000  # Limiter √† 10 000 tweets pour performance\nsample_text = ' '.join(df['text'].dropna().sample(n=sample_size, random_state=42))\nwordcloud = WordCloud(stopwords=stop_words, background_color='white', max_words=200).generate(sample_text)\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud (√âchantillon 10 000 Tweets)')\nplt.show()\n\n# √âtape 2.3 : D√©tection Biais et Qualit√©\n# Explication : Duplicates, NaN, biais (e.g., hype IA dans Generative AI).\nduplicates = df.duplicated(subset=['text']).sum()\nprint(f\"Duplicates : {duplicates} (supprimez si besoin avec df.drop_duplicates()).\")\nnan_count = df.isnull().sum()\nprint(\"NaN par colonne :\", nan_count)\n\n# Biais : V√©rifier si Generative AI a plus positifs (manuellement si target absent)\ngenai_subset = df[df['text'].isin(df_genai['text'].dropna())]  # Tweets de Generative AI\nprint(\"Biais Generative AI : Th√®mes IA souvent positifs (hype). V√©rifiez manuellement si possible.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T13:48:08.595802Z","iopub.execute_input":"2025-11-02T13:48:08.59609Z","iopub.status.idle":"2025-11-02T13:48:16.140904Z","shell.execute_reply.started":"2025-11-02T13:48:08.596066Z","shell.execute_reply":"2025-11-02T13:48:16.139857Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# √âtape 3.1 : Nettoyage du Texte et Suppression Doublons (Optimis√©)\n# Explication : Uniformiser, enlever bruit avec regex l√©ger, supprimer doublons, √©chantillonner.\nimport re\nimport random\nimport pandas as pd\n\n# Supprimer doublons pour am√©liorer qualit√©\ndf = df.drop_duplicates(subset=['text'], inplace=False)  # Nouvelle copie sans doublons\nprint(f\"Doublons supprim√©s ! Nouvelle taille : {len(df)} tweets.\")\n\n# √âchantillonner pour tester (100 000 tweets)\nsample_size = 100000\ndf_sample = df.sample(n=sample_size, random_state=42)\nprint(f\"Travail sur un √©chantillon de {sample_size} tweets.\")\n\n# Nettoyage l√©ger sans spaCy (plus rapide)\ndef clean_text_light(text):\n    text = text.lower()  # Minuscules\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # URLs\n    text = re.sub(r'@\\w+', '', text)  # Mentions\n    text = re.sub(r'[^\\w\\s]', '', text)  # Ponctuation/emojis\n    return text  # Pas de lemmatization\n\ndf_sample['clean_text'] = df_sample['text'].apply(clean_text_light)\nprint(\"Nettoyage l√©ger fait ! Aper√ßu :\", df_sample[['text', 'clean_text']].head())\n\n# √âtape 3.2 : Augmentation des Donn√©es (Optimis√©e et Corrig√©e)\n# Explication : Ajouter donn√©es sur un sous-ensemble de n√©gatifs, g√©rer cha√Ænes vides.\nfrom nltk.corpus import wordnet\nnltk.download('wordnet')\nnltk.download('omw-1.4')\n\ndef synonym_replacement(text, n=2):\n    if not text or not text.strip():  # Si texte vide ou seulement espaces\n        return text\n    words = text.split()\n    for _ in range(n):\n        idx = random.randint(0, len(words)-1)\n        synonyms = wordnet.synsets(words[idx])\n        if synonyms:\n            words[idx] = synonyms[0].lemmas()[0].name()\n    return ' '.join(words)\n\ndef random_swap(text):\n    if not text or not text.strip():  # Si texte vide ou seulement espaces\n        return text\n    words = text.split()\n    if len(words) > 1:\n        idx1, idx2 = random.sample(range(len(words)), 2)\n        words[idx1], words[idx2] = words[idx2], words[idx1]\n    return ' '.join(words)\n\n# Appliquer sur 10 000 n√©gatifs (au lieu de tous), filtrer les vides\naug_df = df_sample[df_sample['target'] == 0].sample(n=10000, random_state=42).copy()\naug_df = aug_df[aug_df['clean_text'].str.strip().astype(bool)]  # Filtrer cha√Ænes vides\naug_df['clean_text'] = aug_df['clean_text'].apply(synonym_replacement).apply(random_swap)\n\n# Combiner\naugmented_df = pd.concat([df_sample, aug_df])\nprint(\"Avant augmentation :\", df_sample['target'].value_counts(normalize=True))\nprint(\"Apr√®s augmentation :\", augmented_df['target'].value_counts(normalize=True))\n\n# √âtape 3.3 : Validation et Sauvegarde (Corrig√©)\n# Explication : Split sur √©chantillon, filtrer None, sauvegarder.\nfrom sklearn.model_selection import train_test_split\n\n# Filtrer les lignes o√π target n'est pas None\naugmented_df = augmented_df[augmented_df['target'].notna()]\nX = augmented_df['clean_text']\ny = augmented_df['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n\nprint(\"Distribution train :\", y_train.value_counts(normalize=True))\nprint(\"Distribution test :\", y_test.value_counts(normalize=True))\n\naugmented_df.to_csv('augmented_tweets_sample.csv', index=False)\nprint(\"Dataset √©chantillon pr√™t et sauvegard√© ! √âthique : Biais r√©duit, anonymis√©.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T13:48:28.505201Z","iopub.execute_input":"2025-11-02T13:48:28.506314Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# √âtape 3.1 : Importation des biblioth√®ques et configuration initiale\n# ---------------------------------------------------------------\n\nimport re\nimport string\nimport nltk\nimport emoji\nfrom collections import Counter\n\n# Tentative silencieuse sans bloquer ni afficher d‚Äôerreur\ntry:\n    # Test rapide : v√©rifier si WordNet est d√©j√† install√© localement\n    nltk.data.find('corpora/wordnet')\n    nltk.data.find('corpora/omw-1.4')\n    from nltk.stem import WordNetLemmatizer\n    lemmatizer = WordNetLemmatizer()\n    use_lemmatization = True\n    print(\" WordNet disponible (lemmatisation activ√©e).\")\n\nexcept LookupError:\n    # Si les fichiers ne sont pas trouv√©s, on passe au stemming\n    from nltk.stem import PorterStemmer\n    lemmatizer = PorterStemmer()\n    use_lemmatization = False\n    print(\" WordNet non disponible (utilisation du stemming Porter).\")\n\n# Initialisation du tokenizer sp√©cialis√© pour les tweets\ntry:\n    from nltk.tokenize import TweetTokenizer\n    tweet_tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n    print(\" TweetTokenizer initialis√©.\")\nexcept:\n    print(\" TweetTokenizer non disponible, utilisation de tokenisation basique.\")\n    tweet_tokenizer = lambda text: text.split()\n\n# Stopwords manuels complets\nstop_words = {\n    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\",\n    \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n    'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself',\n    'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',\n    'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\",\n    'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n    'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an',\n    'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of',\n    'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through',\n    'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n    'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once'\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:05:37.188266Z","iopub.execute_input":"2025-11-02T15:05:37.188677Z","iopub.status.idle":"2025-11-02T15:05:37.203511Z","shell.execute_reply.started":"2025-11-02T15:05:37.188648Z","shell.execute_reply":"2025-11-02T15:05:37.202427Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# √âtape 3.2 : Dictionnaires pour la normalisation du langage informel\n# ---------------------------------------------------------------\n\n# Dictionnaire des contractions anglaises\ncontractions_dict = {\n    \"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\",\n    \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n    \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\", \"he'd've\": \"he would have\",\n    \"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'll\": \"how will\",\n    \"how's\": \"how is\", \"i'd\": \"i would\", \"i'll\": \"i will\", \"i'm\": \"i am\", \"i've\": \"i have\",\n    \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'll\": \"it will\", \"it's\": \"it is\",\n    \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\n    \"must've\": \"must have\", \"mustn't\": \"must not\", \"needn't\": \"need not\",\n    \"oughtn't\": \"ought not\", \"shan't\": \"shall not\", \"she'd\": \"she would\",\n    \"she'll\": \"she will\", \"she's\": \"she is\", \"should've\": \"should have\",\n    \"shouldn't\": \"should not\", \"that's\": \"that is\", \"there's\": \"there is\",\n    \"they'd\": \"they would\", \"they'll\": \"they will\", \"they're\": \"they are\",\n    \"they've\": \"they have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n    \"we'll\": \"we will\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n    \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\",\n    \"where's\": \"where is\", \"who'll\": \"who will\", \"who's\": \"who is\",\n    \"won't\": \"will not\", \"would've\": \"would have\", \"wouldn't\": \"would not\",\n    \"you'd\": \"you would\", \"you'll\": \"you will\", \"you're\": \"you are\", \"you've\": \"you have\"\n}\n\n# Dictionnaire des abr√©viations Twitter courantes\ntwitter_abbreviations = {\n    \"rt\": \"retweet\", \"dm\": \"direct message\", \"tbt\": \"throwback thursday\",\n    \"ama\": \"ask me anything\", \"tldr\": \"too long didn't read\", \"imo\": \"in my opinion\",\n    \"imho\": \"in my humble opinion\", \"nsfw\": \"not safe for work\", \"ftw\": \"for the win\",\n    \"smh\": \"shaking my head\", \"idk\": \"i don't know\", \"brb\": \"be right back\",\n    \"afaik\": \"as far as i know\", \"irl\": \"in real life\", \"fyi\": \"for your information\",\n    \"yolo\": \"you only live once\", \"omg\": \"oh my god\", \"lol\": \"laugh out loud\",\n    \"wtf\": \"what the fuck\", \"tbh\": \"to be honest\", \"bfn\": \"bye for now\"\n}\n\n# Dictionnaire pour la normalisation des √©motions\nemotion_normalization = {\n    \"‚ù§Ô∏è\": \" love \", \"üòç\": \" love \", \"üòä\": \" happy \", \"üòÇ\": \" laugh \", \"üò≠\": \" cry \",\n    \"üò¢\": \" sad \", \"üò°\": \" angry \", \"ü§î\": \" thinking \", \"üòé\": \" cool \", \"üî•\": \" fire \",\n    \"üëç\": \" good \", \"üëé\": \" bad \", \"üéâ\": \" celebrate \", \"ü§Ø\": \" mind blown \",\n    \"üíØ\": \" hundred percent \", \"‚ú®\": \" sparkle \", \"üíï\": \" love \", \"üíî\": \" broken heart \"\n}\n\n# Dictionnaire pour les √©motic√¥nes textuelles\ntext_emoticons = {\n    \":)\": \" smile \", \":-)\": \" smile \", \": )\": \" smile \", \":D\": \" big smile \",\n    \":-D\": \" big smile \", \":(\": \" sad \", \":-(\": \" sad \", \": (\": \" sad \",\n    \":'(\": \" cry \", \";-)\": \" wink \", \"; )\": \" wink \", \";)\": \" wink \",\n    \":P\": \" tongue \", \":-P\": \" tongue \", \": P\": \" tongue \", \"XP\": \" tongue \",\n    \"xD\": \" laughing \", \"XD\": \" laughing \", \"<3\": \" love \", \"</3\": \" broken heart \"\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:08:19.059544Z","iopub.execute_input":"2025-11-02T15:08:19.060531Z","iopub.status.idle":"2025-11-02T15:08:19.07143Z","shell.execute_reply.started":"2025-11-02T15:08:19.060499Z","shell.execute_reply":"2025-11-02T15:08:19.070241Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# √âtape 3.1 : Fonctions de nettoyage de base\n# ---------------------------------------------------------------\n\ndef remove_urls(text):\n    \"\"\"Supprime toutes les URLs du texte\"\"\"\n    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url_pattern.sub('', text)\n\ndef remove_mentions_and_hashtags(text):\n    \"\"\"Supprime les mentions @utilisateur et les hashtags #motcl√©\"\"\"\n    text = re.sub(r'@\\w+', '', text)  # Supprime @utilisateur\n    text = re.sub(r'#(\\w+)', r'\\1', text)  # Garde seulement le texte du hashtag\n    return text\n\ndef handle_punctuation(text):\n    \"\"\"Gestion de la ponctuation et caract√®res sp√©ciaux\"\"\"\n    # Garde la ponctuation sentimentale\n    text = re.sub(r'!+', ' ! ', text)\n    text = re.sub(r'\\?+', ' ? ', text)\n    \n    # Supprime les autres caract√®res sp√©ciaux\n    text = re.sub(r'[^\\w\\s!?]', '', text)\n    \n    return text\n\ndef to_lowercase(text):\n    \"\"\"Conversion en minuscules\"\"\"\n    return text.lower()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:08:35.739428Z","iopub.execute_input":"2025-11-02T15:08:35.739745Z","iopub.status.idle":"2025-11-02T15:08:35.74724Z","shell.execute_reply.started":"2025-11-02T15:08:35.739721Z","shell.execute_reply":"2025-11-02T15:08:35.746243Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# √âtape 3.2 : Fonctions de normalisation avanc√©e\n# ---------------------------------------------------------------\n\ndef reduce_character_repetition(text):\n    \"\"\"Correction des r√©p√©titions de caract√®res (ex: 'soooo' -> 'so')\"\"\"\n    # Pattern pour d√©tecter les r√©p√©titions de 3 caract√®res ou plus\n    pattern = re.compile(r\"(.)\\1{2,}\")\n    return pattern.sub(r\"\\1\\1\", text)  # R√©duit √† 2 r√©p√©titions maximum\n\ndef normalize_emojis_and_emoticons(text):\n    \"\"\"Gestion des √©motic√¥nes et emojis\"\"\"\n    # Convertit les emojis en texte\n    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n    \n    # Remplace les emojis par leur signification\n    for emoji_char, meaning in emotion_normalization.items():\n        text = text.replace(emoji_char, meaning)\n    \n    # Remplace les √©motic√¥nes textuelles\n    for emoticon, meaning in text_emoticons.items():\n        text = text.replace(emoticon, meaning)\n    \n    return text\n\ndef expand_contractions(text):\n    \"\"\"Normalisation des contractions (ex: 'don't' -> 'do not')\"\"\"\n    contractions_pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())), \n                                    flags=re.IGNORECASE|re.DOTALL)\n    \n    def expand_match(contraction):\n        match = contraction.group(0)\n        expanded = contractions_dict.get(match.lower())\n        return expanded if expanded else match\n    \n    return contractions_pattern.sub(expand_match, text)\n\ndef expand_twitter_abbreviations(text):\n    \"\"\"Traitement des abr√©viations Twitter\"\"\"\n    words = text.split()\n    normalized_words = []\n    \n    for word in words:\n        normalized_word = twitter_abbreviations.get(word.lower(), word)\n        normalized_words.append(normalized_word)\n    \n    return ' '.join(normalized_words)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:08:51.846277Z","iopub.execute_input":"2025-11-02T15:08:51.846615Z","iopub.status.idle":"2025-11-02T15:08:51.855689Z","shell.execute_reply.started":"2025-11-02T15:08:51.846591Z","shell.execute_reply":"2025-11-02T15:08:51.854705Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# √âtape 3.3 : Tokenization sp√©cialis√©e\n# ---------------------------------------------------------------\n\ndef sentiment_aware_tokenization(text):\n    \"\"\"Tokenisation qui pr√©serve les ponctuations sentimentales\"\"\"\n    try:\n        # Utilise le tokenizer sp√©cialis√© tweets si disponible\n        if 'tweet_tokenizer' in globals():\n            tokens = tweet_tokenizer.tokenize(text)\n        else:\n            # Tokenisation manuelle\n            tokens = re.findall(r'\\b\\w+\\b|[!?]+', text)\n        \n        return tokens\n    except Exception as e:\n        print(f\"Erreur tokenisation: {e}\")\n        return []\n\ndef remove_stopwords_and_process(tokens):\n    \"\"\"Supprime les stopwords et applique lemmatisation/stemming\"\"\"\n    processed_tokens = []\n    \n    for token in tokens:\n        # Garde les tokens de ponctuation sentimentale\n        if token in ['!', '?']:\n            processed_tokens.append(token)\n        # Supprime les stopwords et applique le traitement\n        elif token not in stop_words and len(token) > 2:\n            try:\n                if use_lemmatization:\n                    if hasattr(lemmatizer, 'lemmatize'):\n                        # Lemmatisation\n                        processed_token = lemmatizer.lemmatize(token)\n                    else:\n                        # Stemming\n                        processed_token = lemmatizer.stem(token)\n                else:\n                    processed_token = token\n                processed_tokens.append(processed_token)\n            except:\n                processed_tokens.append(token)\n    \n    return processed_tokens\n\ndef create_special_tokens(text):\n    \"\"\"Cr√©e des tokens sp√©ciaux pour caract√©ristiques importantes\"\"\"\n    special_tokens = []\n    \n    # D√©tection de la pr√©sence d'URL\n    if re.search(r'http\\S+|www\\.\\S+', text):\n        special_tokens.append('[URL]')\n    \n    # D√©tection de la pr√©sence de mentions\n    if re.search(r'@\\w+', text):\n        special_tokens.append('[MENTION]')\n    \n    # D√©tection de la pr√©sence de hashtags\n    if re.search(r'#\\w+', text):\n        special_tokens.append('[HASHTAG]')\n    \n    # D√©tection de l'enthousiasme\n    if re.search(r'!{2,}', text):\n        special_tokens.append('[EXCITED]')\n    \n    # D√©tection des questions\n    if re.search(r'\\?{2,}', text):\n        special_tokens.append('[QUESTION]')\n    \n    return special_tokens\n\ndef handle_informal_language(text):\n    \"\"\"Gestion du langage informel sp√©cifique aux tweets\"\"\"\n    # Remplace les formes courantes de langage informel\n    informal_patterns = {\n        r'\\b(u)\\b': 'you',\n        r'\\b(ur)\\b': 'your',\n        r'\\b(plz|pls)\\b': 'please',\n        r'\\b(thx|thanx)\\b': 'thanks',\n        r'\\b(gonna)\\b': 'going to',\n        r'\\b(wanna)\\b': 'want to',\n        r'\\b(gotta)\\b': 'got to'\n    }\n    \n    for pattern, replacement in informal_patterns.items():\n        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n    \n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:09:08.539398Z","iopub.execute_input":"2025-11-02T15:09:08.539718Z","iopub.status.idle":"2025-11-02T15:09:08.551762Z","shell.execute_reply.started":"2025-11-02T15:09:08.539694Z","shell.execute_reply":"2025-11-02T15:09:08.550709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================================================\n# √âTAPE 3.5 : PR√âPARATION DU DATAFRAME POUR L'ANALYSE\n# ===============================================================\n\nimport pandas as pd\nfrom collections import Counter\n\n# Si tu as d√©j√† un DataFrame initial appel√© df_raw ou df (avec une colonne \"text\")\n# on cr√©e une copie et on la pr√©pare\nif 'df_processed' not in locals():\n    df_processed = df.copy() if 'df' in locals() else pd.DataFrame()\n\n# V√©rifie que la colonne de texte existe\nif 'text' not in df_processed.columns:\n    raise ValueError(\"‚ö†Ô∏è La colonne 'text' est introuvable dans ton DataFrame. V√©rifie le nom exact.\")\n\n# Supposons que tu as d√©j√† une fonction 'clean_tweet' de ton √©tape 3\ndef clean_tweet(text):\n    import re, string, emoji\n    from nltk.tokenize import TweetTokenizer\n    from nltk.stem import PorterStemmer\n    \n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n    stemmer = PorterStemmer()\n    \n    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)  # liens\n    text = re.sub(r'\\@\\w+|\\#','', text)  # mentions et hashtags\n    text = emoji.replace_emoji(text, replace='')  # supprime les √©mojis\n    text = text.translate(str.maketrans('', '', string.punctuation))  # ponctuation\n    \n    tokens = tokenizer.tokenize(text)\n    tokens = [stemmer.stem(word) for word in tokens if word.isalpha()]\n    return \" \".join(tokens)\n\n# Appliquer le nettoyage\ndf_processed['cleaned_text'] = df_processed['text'].astype(str).apply(clean_tweet)\n\nprint(\"‚úÖ Texte nettoy√© et stock√© dans la colonne 'cleaned_text'\")\nprint(df_processed[['text', 'cleaned_text']].head())\n\n# Copier pour analyse\ndf_analysis = df_processed.copy()\nhas_sentiment = 'target' in df_analysis.columns\n\nprint(\"\\nüìä Donn√©es pr√™tes pour analyse :\")\nprint(f\"- {len(df_analysis):,} lignes\")\nprint(f\"- Colonnes disponibles : {list(df_analysis.columns)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:24:51.334275Z","iopub.execute_input":"2025-11-02T15:24:51.335274Z","iopub.status.idle":"2025-11-02T15:35:22.773864Z","shell.execute_reply.started":"2025-11-02T15:24:51.335238Z","shell.execute_reply":"2025-11-02T15:35:22.772717Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Application du pr√©traitement complet\nprint(\"D√©but du pr√©traitement complet des tweets...\")\n\n# Utiliser un √©chantillon pour tester\nsample_size = 10000\ndf_sample = df.sample(n=min(sample_size, len(df)), random_state=42)\n\n# Appliquer le pr√©traitement\ndf_sample['cleaned_text'] = df_sample['text'].apply(preprocess_tweet)\n\n# V√©rification\nprint(f\"Pr√©traitement termin√© pour {len(df_sample)} tweets\")\nprint(f\"Textes vides: {(df_sample['cleaned_text'].str.strip() == '').sum()}\")\n\n# Afficher des exemples\nprint(\"\\n\" + \"=\"*60)\nprint(\"EXEMPLES DE PR√âTRAITEMENT COMPLET\")\nprint(\"=\"*60)\n\nfor i in range(3):\n    original = df_sample['text'].iloc[i]\n    cleaned = df_sample['cleaned_text'].iloc[i]\n    \n    print(f\"\\nüéØ EXEMPLE {i+1}:\")\n    print(f\"AVANT:  {original}\")\n    print(f\"APR√àS:  {cleaned}\")\n    print(f\"R√âDUCTION: {len(original)} ‚Üí {len(cleaned)} caract√®res\")\n    print(\"-\" * 80)\n\nprint(\"\\n‚úÖ √âTAPE 3 TERMIN√âE AVEC SUCC√àS!\")\nprint(\"Tous les √©l√©ments de pr√©traitement sont impl√©ment√©s:\")\nprint(\"‚úì 3.1 Nettoyage de base\")\nprint(\"‚úì 3.2 Normalisation avanc√©e\") \nprint(\"‚úì 3.3 Tokenization sp√©cialis√©e\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:35:34.054065Z","iopub.execute_input":"2025-11-02T15:35:34.054399Z","iopub.status.idle":"2025-11-02T15:35:37.624991Z","shell.execute_reply.started":"2025-11-02T15:35:34.054374Z","shell.execute_reply":"2025-11-02T15:35:37.623948Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# √âtape 3.8 : Application du pr√©traitement COMPLET sur vos datasets\n# ---------------------------------------------------------------\n\nprint(\"üîÑ APPLICATION DU PR√âTRAITEMENT SUR VOS DONN√âES R√âELLES...\")\n\n# Reprendre le dataset combin√© original (1.6M+ tweets)\nprint(f\"Taille du dataset combin√©: {len(df)} tweets\")\n\n# Strat√©gie pour g√©rer la grande taille du dataset\ndef process_in_batches(df, batch_size=50000):\n    \"\"\"Traite le dataset par lots pour √©viter les probl√®mes de m√©moire\"\"\"\n    total_batches = len(df) // batch_size + 1\n    processed_texts = []\n    \n    for i in range(total_batches):\n        start_idx = i * batch_size\n        end_idx = min((i + 1) * batch_size, len(df))\n        \n        print(f\"Traitement du lot {i+1}/{total_batches} (tweets {start_idx}-{end_idx})\")\n        \n        batch = df.iloc[start_idx:end_idx]\n        batch_processed = batch['text'].apply(preprocess_tweet)\n        processed_texts.extend(batch_processed)\n        \n        # Lib√©rer m√©moire\n        del batch\n        del batch_processed\n    \n    return processed_texts\n\n# Appliquer le pr√©traitement sur un √©chantillon d'abord (pour test)\nsample_size = 50000  # Augmenter la taille pour plus de donn√©es\nprint(f\"üîç Pr√©traitement d'un √©chantillon de {sample_size} tweets pour test...\")\n\ndf_sample_large = df.sample(n=min(sample_size, len(df)), random_state=42)\ndf_sample_large['cleaned_text'] = df_sample_large['text'].apply(preprocess_tweet)\n\nprint(\"‚úÖ Pr√©traitement de l'√©chantillon termin√©!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:10:37.575759Z","iopub.execute_input":"2025-11-02T15:10:37.576096Z","iopub.status.idle":"2025-11-02T15:10:54.482951Z","shell.execute_reply.started":"2025-11-02T15:10:37.576072Z","shell.execute_reply":"2025-11-02T15:10:54.481934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# √âtape 3.9 : Analyse d√©taill√©e sur vos donn√©es pr√©trait√©es\n# ---------------------------------------------------------------\n\ndef analyze_actual_preprocessing(df_processed):\n    \"\"\"Analyse approfondie du pr√©traitement sur vos donn√©es r√©elles\"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(\"üìä ANALYSE DU PR√âTRAITEMENT SUR VOS DONN√âES R√âELLES\")\n    print(\"=\"*70)\n    \n    # 1. Statistiques g√©n√©rales\n    total_tweets = len(df_processed)\n    successful_cleaning = df_processed['cleaned_text'].notna().sum()\n    empty_texts = (df_processed['cleaned_text'].str.strip() == '').sum()\n    \n    print(f\"üìà STATISTIQUES G√âN√âRALES:\")\n    print(f\"   - Total tweets: {total_tweets:,}\")\n    print(f\"   - Tweets trait√©s avec succ√®s: {successful_cleaning:,} ({successful_cleaning/total_tweets*100:.1f}%)\")\n    print(f\"   - Textes vides apr√®s nettoyage: {empty_texts:,} ({empty_texts/total_tweets*100:.1f}%)\")\n    \n    # 2. Analyse de r√©duction\n    original_lengths = df_processed['text'].str.len()\n    cleaned_lengths = df_processed['cleaned_text'].str.len()\n    \n    print(f\"\\nüìè ANALYSE DE R√âDUCTION:\")\n    print(f\"   - Longueur moyenne avant: {original_lengths.mean():.1f} caract√®res\")\n    print(f\"   - Longueur moyenne apr√®s: {cleaned_lengths.mean():.1f} caract√®res\")\n    print(f\"   - R√©duction: {((original_lengths.mean() - cleaned_lengths.mean()) / original_lengths.mean() * 100):.1f}%\")\n    \n    # 3. Distribution des sentiments (Sentiment140 seulement)\n    if 'target' in df_processed.columns:\n        sentiment_stats = df_processed.dropna(subset=['target'])['target'].value_counts()\n        print(f\"\\nüé≠ DISTRIBUTION DES SENTIMENTS (Sentiment140):\")\n        for sentiment, count in sentiment_stats.items():\n            sentiment_label = \"N√©gatif\" if sentiment == 0 else \"Positif\" if sentiment == 4 else f\"Classe {sentiment}\"\n            percentage = (count / len(df_processed.dropna(subset=['target']))) * 100\n            print(f\"   - {sentiment_label}: {count:,} tweets ({percentage:.1f}%)\")\n    \n    # 4. Analyse des tokens sp√©ciaux\n    def count_special_tokens(text):\n        specials = 0\n        if '[URL]' in text: specials += 1\n        if '[MENTION]' in text: specials += 1\n        if '[HASHTAG]' in text: specials += 1\n        if '[EXCITED]' in text: specials += 1\n        if '[QUESTION]' in text: specials += 1\n        return specials\n    \n    special_counts = df_processed['cleaned_text'].apply(count_special_tokens)\n    print(f\"\\nüî§ TOKENS SP√âCIAUX D√âTECT√âS:\")\n    print(f\"   - Tweets avec tokens sp√©ciaux: {(special_counts > 0).sum():,}\")\n    print(f\"   - Moyenne de tokens sp√©ciaux par tweet: {special_counts.mean():.2f}\")\n    \n    # 5. Exemples concrets de vos donn√©es\n    print(f\"\\nüîç EXEMPLES CONCRETS DE VOS DONN√âES:\")\n    \n    # Trouver des exemples int√©ressants\n    examples_to_show = 5\n    shown = 0\n    for idx, row in df_processed.iterrows():\n        if shown >= examples_to_show:\n            break\n        \n        original = row['text']\n        cleaned = row['cleaned_text']\n        \n        # Montrer seulement si le pr√©traitement a fait une diff√©rence significative\n        if len(original) > 50 and len(cleaned) > 10 and len(original) != len(cleaned):\n            print(f\"\\n--- Exemple {shown + 1} ---\")\n            print(f\"AVANT:  {original[:150]}{'...' if len(original) > 150 else ''}\")\n            print(f\"APR√àS:  {cleaned[:150]}{'...' if len(cleaned) > 150 else ''}\")\n            print(f\"R√âDUCTION: {len(original)} ‚Üí {len(cleaned)} caract√®res\")\n            shown += 1\n\n# Appliquer l'analyse sur votre √©chantillon\nanalyze_actual_preprocessing(df_sample_large)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:11:14.579244Z","iopub.execute_input":"2025-11-02T15:11:14.580108Z","iopub.status.idle":"2025-11-02T15:11:14.74583Z","shell.execute_reply.started":"2025-11-02T15:11:14.580068Z","shell.execute_reply":"2025-11-02T15:11:14.744839Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# √âtape 3.10 : Pr√©paration finale pour la mod√©lisation avec vos donn√©es\n# ---------------------------------------------------------------\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"üöÄ PR√âPARATION POUR LES √âTAPES SUIVANTES AVEC VOS DONN√âES\")\nprint(\"=\"*70)\n\n# 1. Nettoyer les donn√©es finales\ndf_final = df_sample_large[df_sample_large['cleaned_text'].str.strip() != ''].copy()\nprint(f\"‚úÖ Donn√©es finales apr√®s filtrage: {len(df_final):,} tweets\")\n\n# 2. S√©parer les donn√©es Sentiment140 (avec labels) et GenerativeAI (sans labels)\nif 'target' in df_final.columns:\n    df_labeled = df_final.dropna(subset=['target']).copy()\n    df_unlabeled = df_final[df_final['target'].isna()].copy()\n    \n    print(f\"\\nüìä S√âPARATION DES DONN√âES:\")\n    print(f\"   - Sentiment140 (avec labels): {len(df_labeled):,} tweets\")\n    print(f\"   - GenerativeAI (sans labels): {len(df_unlabeled):,} tweets\")\n    \n    # Distribution des sentiments pour l'analyse\n    sentiment_dist = df_labeled['target'].value_counts()\n    print(f\"\\nüéØ DISTRIBUTION POUR L'APPRENTISSAGE:\")\n    for sentiment, count in sentiment_dist.items():\n        label = \"N√©gatif\" if sentiment == 0 else \"Positif\"\n        print(f\"   - {label}: {count:,} tweets\")\nelse:\n    df_labeled = pd.DataFrame()\n    df_unlabeled = df_final.copy()\n    print(\"‚ö†Ô∏è  Aucune donn√©e √©tiquet√©e trouv√©e\")\n\n# 3. Sauvegarder les donn√©es pr√©trait√©es\ntry:\n    # Sauvegarder l'√©chantillon pr√©trait√©\n    df_final.to_csv('/kaggle/working/cleaned_tweets_sample.csv', index=False)\n    \n    if len(df_labeled) > 0:\n        df_labeled.to_csv('/kaggle/working/cleaned_labeled_tweets.csv', index=False)\n    \n    if len(df_unlabeled) > 0:\n        df_unlabeled.to_csv('/kaggle/working/cleaned_unlabeled_tweets.csv', index=False)\n    \n    print(f\"\\nüíæ DONN√âES SAUVEGARD√âES:\")\n    print(f\"   - /kaggle/working/cleaned_tweets_sample.csv\")\n    if len(df_labeled) > 0:\n        print(f\"   - /kaggle/working/cleaned_labeled_tweets.csv\")\n    if len(df_unlabeled) > 0:\n        print(f\"   - /kaggle/working/cleaned_unlabeled_tweets.csv\")\n    \nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  Erreur sauvegarde: {e}\")\n\n# 4. R√©sum√© final\nprint(\"\\n\" + \"=\"*70)\nprint(\"üéâ √âTAPE 3 TERMIN√âE - R√âSUM√â FINAL\")\nprint(\"=\"*70)\nprint(f\"üì¶ Donn√©es sources:\")\nprint(f\"   - Sentiment140: 1,600,000 tweets (avec sentiments)\")\nprint(f\"   - GenerativeAI: {len(df_genai):,} tweets (IA g√©n√©rative)\")\nprint(f\"   - Total combin√©: {len(df):,} tweets\")\n\nprint(f\"\\nüîß Pr√©traitement appliqu√©:\")\nprint(f\"   - √âchantillon trait√©: {len(df_sample_large):,} tweets\")\nprint(f\"   - Donn√©es finales: {len(df_final):,} tweets\")\nprint(f\"   - Taux de succ√®s: {(len(df_final)/len(df_sample_large)*100):.1f}%\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:12:21.738497Z","iopub.execute_input":"2025-11-02T15:12:21.738808Z","iopub.status.idle":"2025-11-02T15:12:22.431538Z","shell.execute_reply.started":"2025-11-02T15:12:21.738783Z","shell.execute_reply":"2025-11-02T15:12:22.430576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================================================\n# √âTAPE 4 : ANALYSE EXPLORATOIRE APPROFONDIE (version robuste)\n# ===============================================================\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom wordcloud import WordCloud\nfrom collections import Counter, defaultdict\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nprint(\"üöÄ D√âBUT DE L'ANALYSE EXPLORATOIRE APPROFONDIE\")\nprint(\"=\"*60)\n\n# ---------------------------------------------------------------\n# 1Ô∏è‚É£ V√©rifier si les donn√©es pr√©trait√©es existent d√©j√†\n# ---------------------------------------------------------------\n\ndef get_or_create_df_processed():\n    # Si df_processed existe d√©j√†\n    if 'df_processed' in globals():\n        print(\"‚úî Utilisation de df_processed d√©j√† pr√©sent.\")\n        return globals()['df_processed']\n    \n    # Sinon, essayer df ou df_raw\n    if 'df' in globals():\n        print(\"‚úî 'df' trouv√© ‚Üí copie vers df_processed.\")\n        return globals()['df'].copy()\n    \n    if 'df_raw' in globals():\n        print(\"‚úî 'df_raw' trouv√© ‚Üí copie vers df_processed.\")\n        return globals()['df_raw'].copy()\n    \n    # Sinon, essayer de charger un CSV automatiquement\n    default_paths = [\"data.csv\", \"tweets.csv\", \"dataset.csv\"]\n    for p in default_paths:\n        if os.path.exists(p):\n            print(f\"‚úî Chargement automatique depuis {p}\")\n            return pd.read_csv(p)\n    \n    # Sinon, demander manuellement le chemin\n    path = input(\"‚ùì Aucun DataFrame trouv√©. Entrez le chemin de votre fichier CSV : \")\n    if os.path.exists(path):\n        print(f\"‚úî Chargement manuel depuis {path}\")\n        return pd.read_csv(path)\n    else:\n        raise FileNotFoundError(\"‚ö†Ô∏è Aucun fichier CSV trouv√©. V√©rifie ton chemin ou ex√©cute d'abord l'√©tape 3.\")\n\n# R√©cup√©ration du DataFrame\ndf_processed = get_or_create_df_processed()\n\n# ---------------------------------------------------------------\n# 2Ô∏è‚É£ Pr√©paration des donn√©es pour l‚Äôanalyse\n# ---------------------------------------------------------------\ndf_analysis = df_processed.copy()\n\n# V√©rifier la colonne de sentiment\nhas_sentiment = 'target' in df_analysis.columns\n\nprint(f\"üìä Donn√©es disponibles : {len(df_analysis):,} tweets\")\nprint(f\"üéØ Donn√©es de sentiment disponibles : {has_sentiment}\")\nprint(\"=\"*60)\n\n# ---------------------------------------------------------------\n# 3Ô∏è‚É£ Premiers aper√ßus\n# ---------------------------------------------------------------\nprint(\"üîé Aper√ßu des donn√©es :\")\ndisplay(df_analysis.head())\n\nprint(\"\\nüß© Colonnes disponibles :\")\nprint(df_analysis.columns.tolist())\n\nprint(\"\\nüßπ Valeurs manquantes :\")\nprint(df_analysis.isna().sum())\n\n# ---------------------------------------------------------------\n# 4Ô∏è‚É£ Visualisation de base\n# ---------------------------------------------------------------\nif has_sentiment:\n    plt.figure(figsize=(6,4))\n    sns.countplot(data=df_analysis, x='target', palette='Set2')\n    plt.title(\"R√©partition des sentiments (0 = n√©gatif, 4 = positif)\")\n    plt.xlabel(\"Sentiment\")\n    plt.ylabel(\"Nombre de tweets\")\n    plt.show()\nelse:\n    print(\"‚ö†Ô∏è Colonne 'target' non trouv√©e ‚Äî aucune visualisation de sentiments possible.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:19:26.619856Z","iopub.execute_input":"2025-11-02T15:19:26.620187Z","iopub.status.idle":"2025-11-02T15:19:30.608332Z","shell.execute_reply.started":"2025-11-02T15:19:26.620165Z","shell.execute_reply":"2025-11-02T15:19:30.60739Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4.1 ANALYSE STATISTIQUE\n# ===============================================================\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"4.1 ANALYSE STATISTIQUE\")\nprint(\"=\"*50)\n\n# Distribution des longueurs\ndf_analysis['text_length'] = df_analysis['cleaned_text'].str.len()\ndf_analysis['word_count'] = df_analysis['cleaned_text'].str.split().str.len()\n\nprint(\"üìè DISTRIBUTION DES LONGUEURS:\")\nprint(f\"   - Longueur moyenne : {df_analysis['text_length'].mean():.1f} caract√®res\")\nprint(f\"   - Mots moyens : {df_analysis['word_count'].mean():.1f}\")\nprint(f\"   - Min/Max : {df_analysis['text_length'].min()} / {df_analysis['text_length'].max()} caract√®res\")\n\n# Mots les plus fr√©quents\nall_words = ' '.join(df_analysis['cleaned_text']).split()\nword_freq = Counter(all_words)\ntop_words = word_freq.most_common(20)\n\nprint(f\"\\nüî§ MOTS LES PLUS FR√âQUENTS :\")\nfor i, (word, freq) in enumerate(top_words[:10], 1):\n    print(f\"   {i:2d}. {word:15} : {freq:6,}\")\n\n# R√©partition des sentiments\nif has_sentiment:\n    print(f\"\\nüé≠ R√âPARTITION DES SENTIMENTS:\")\n    sentiment_counts = df_analysis['target'].value_counts()\n    for sentiment, count in sentiment_counts.items():\n        label = \"N√©gatif\" if sentiment == 0 else \"Positif\"\n        pct = count / len(df_analysis) * 100\n        print(f\"   - {label}: {count:,} tweets ({pct:.1f}%)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:38:14.284851Z","iopub.execute_input":"2025-11-02T15:38:14.285561Z","iopub.status.idle":"2025-11-02T15:38:29.548703Z","shell.execute_reply.started":"2025-11-02T15:38:14.285529Z","shell.execute_reply":"2025-11-02T15:38:29.547667Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4.2 VISUALISATIONS AVANC√âES\n# ===============================================================\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"4.2 VISUALISATIONS AVANC√âES\")\nprint(\"=\"*50)\n\n# Configuration des styles\nplt.style.use('default')\nsns.set_palette(\"husl\")\n\n# Cr√©er une figure avec plusieurs subplots\nfig = plt.figure(figsize=(20, 15))\n\n# 1. Distribution des longueurs de textes\nplt.subplot(2, 3, 1)\nplt.hist(df_analysis['text_length'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\nplt.xlabel('Longueur du texte (caract√®res)')\nplt.ylabel('Fr√©quence')\nplt.title('Distribution des Longueurs de Textes')\nplt.grid(True, alpha=0.3)\n\n# 2. Distribution du nombre de mots\nplt.subplot(2, 3, 2)\nplt.hist(df_analysis['word_count'], bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\nplt.xlabel('Nombre de mots')\nplt.ylabel('Fr√©quence')\nplt.title('Distribution du Nombre de Mots par Tweet')\nplt.grid(True, alpha=0.3)\n\n# 3. R√©partition des sentiments (si disponible)\nif has_sentiment:\n    plt.subplot(2, 3, 3)\n    sentiment_labels = ['N√©gatif', 'Positif']\n    sentiment_values = [sentiment_counts.get(0, 0), sentiment_counts.get(4, 0)]\n    \n    plt.pie(sentiment_values, labels=sentiment_labels, autopct='%1.1f%%', \n            colors=['#ff9999', '#66b3ff'], startangle=90)\n    plt.title('R√©partition des Sentiments')\n\n# 4. Word Cloud g√©n√©ral\nplt.subplot(2, 3, 4)\nwordcloud = WordCloud(width=800, height=400, background_color='white', \n                      max_words=100, colormap='viridis').generate(' '.join(df_analysis['cleaned_text']))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud - Tous les Tweets')\n\n# 5. Word Cloud par sentiment (si disponible)\nif has_sentiment:\n    plt.subplot(2, 3, 5)\n    positive_texts = ' '.join(df_analysis[df_analysis['target'] == 4]['cleaned_text'])\n    wordcloud_positive = WordCloud(width=800, height=400, background_color='white', \n                                  max_words=100, colormap='Greens').generate(positive_texts)\n    plt.imshow(wordcloud_positive, interpolation='bilinear')\n    plt.axis('off')\n    plt.title('Word Cloud - Sentiments Positifs')\n\n    plt.subplot(2, 3, 6)\n    negative_texts = ' '.join(df_analysis[df_analysis['target'] == 0]['cleaned_text'])\n    wordcloud_negative = WordCloud(width=800, height=400, background_color='white', \n                                  max_words=100, colormap='Reds').generate(negative_texts)\n    plt.imshow(wordcloud_negative, interpolation='bilinear')\n    plt.axis('off')\n    plt.title('Word Cloud - Sentiments N√©gatifs')\n\nplt.tight_layout()\nplt.show()\n\n# Analyse temporelle (si donn√©es disponibles)\nif 'date' in df_analysis.columns:\n    print(\"\\nüìÖ ANALYSE TEMPORELLE:\")\n    try:\n        df_analysis['date'] = pd.to_datetime(df_analysis['date'])\n        df_analysis['month'] = df_analysis['date'].dt.to_period('M')\n        \n        monthly_counts = df_analysis['month'].value_counts().sort_index()\n        \n        plt.figure(figsize=(12, 6))\n        monthly_counts.plot(kind='line', marker='o', color='purple')\n        plt.title('√âvolution du Volume de Tweets par Mois')\n        plt.xlabel('Mois')\n        plt.ylabel('Nombre de Tweets')\n        plt.xticks(rotation=45)\n        plt.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.show()\n        \n        print(f\"   - P√©riode couverte: {df_analysis['date'].min()} to {df_analysis['date'].max()}\")\n        print(f\"   - Mois le plus actif: {monthly_counts.idxmax()} ({monthly_counts.max()} tweets)\")\n        \n    except Exception as e:\n        print(f\"   ‚ö†Ô∏è  Impossible d'analyser les dates: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:41:34.716759Z","iopub.execute_input":"2025-11-02T15:41:34.71711Z","iopub.status.idle":"2025-11-02T15:43:49.823963Z","shell.execute_reply.started":"2025-11-02T15:41:34.717086Z","shell.execute_reply":"2025-11-02T15:43:49.822779Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4.3 D√âTECTION DES PATTERNS\n# ===============================================================\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"4.3 D√âTECTION DES PATTERNS\")\nprint(\"=\"*50)\n\n# Patterns linguistiques par sentiment\nif has_sentiment:\n    print(\"\\nüîç PATTERNS LINGUISTIQUES PAR SENTIMENT:\")\n    \n    # S√©parer les textes par sentiment\n    positive_texts = df_analysis[df_analysis['target'] == 4]['cleaned_text']\n    negative_texts = df_analysis[df_analysis['target'] == 0]['cleaned_text']\n    \n    # Mots les plus fr√©quents par sentiment\n    positive_words = ' '.join(positive_texts).split()\n    negative_words = ' '.join(negative_texts).split()\n    \n    positive_freq = Counter(positive_words)\n    negative_freq = Counter(negative_words)\n    \n    # Trouver les mots distinctifs\n    all_positive_words = set(positive_freq.keys())\n    all_negative_words = set(negative_freq.keys())\n    \n    distinctive_positive = all_positive_words - all_negative_words\n    distinctive_negative = all_negative_words - all_positive_words\n    \n    print(\"   Mots distinctifs positifs (Top 10):\")\n    pos_distinctive_words = [(word, positive_freq[word]) for word in distinctive_positive]\n    pos_distinctive_words.sort(key=lambda x: x[1], reverse=True)\n    for word, freq in pos_distinctive_words[:10]:\n        print(f\"      - {word}: {freq} occurrences\")\n    \n    print(\"\\n   Mots distinctifs n√©gatifs (Top 10):\")\n    neg_distinctive_words = [(word, negative_freq[word]) for word in distinctive_negative]\n    neg_distinctive_words.sort(key=lambda x: x[1], reverse=True)\n    for word, freq in neg_distinctive_words[:10]:\n        print(f\"      - {word}: {freq} occurrences\")\n\n# Analyse des n-grams\nprint(\"\\nüìä ANALYSE DES N-GRAMS:\")\n\ndef get_top_ngrams(corpus, n=2, top_k=10):\n    \"\"\"Extrait les n-grams les plus fr√©quents\"\"\"\n    vec = CountVectorizer(ngram_range=(n, n), max_features=top_k)\n    X = vec.fit_transform(corpus)\n    words = vec.get_feature_names_out()\n    counts = X.sum(axis=0).A1\n    return list(zip(words, counts))\n\n# Bigrams\ntry:\n    top_bigrams = get_top_ngrams(df_analysis['cleaned_text'], n=2, top_k=15)\n    print(f\"   Bigrams les plus fr√©quents:\")\n    for i, (bigram, count) in enumerate(top_bigrams[:10], 1):\n        print(f\"      {i:2d}. {bigram:20} : {count:4} occurrences\")\nexcept Exception as e:\n    print(f\"   ‚ö†Ô∏è  Erreur avec les bigrams: {e}\")\n\n# Trigrams\ntry:\n    top_trigrams = get_top_ngrams(df_analysis['cleaned_text'], n=3, top_k=10)\n    print(f\"\\n   Trigrams les plus fr√©quents:\")\n    for i, (trigram, count) in enumerate(top_trigrams[:5], 1):\n        print(f\"      {i:2d}. {trigram:25} : {count:4} occurrences\")\nexcept Exception as e:\n    print(f\"   ‚ö†Ô∏è  Erreur avec les trigrams: {e}\")\n\n# D√©tection des topics dominants\nprint(\"\\nüéØ D√âTECTION DES TOPICS DOMINANTS:\")\n\n# Analyser les mots les plus fr√©quents par cat√©gorie\ndef analyze_topics_by_frequency(texts, category_name, top_n=15):\n    \"\"\"Analyse les topics par fr√©quence des mots\"\"\"\n    all_text = ' '.join(texts)\n    words = all_text.split()\n    word_freq = Counter(words)\n    \n    # Filtrer les mots courts et peu informatifs\n    meaningful_words = [(word, freq) for word, freq in word_freq.items() \n                       if len(word) > 3 and freq > 5]\n    meaningful_words.sort(key=lambda x: x[1], reverse=True)\n    \n    return meaningful_words[:top_n]\n\n# Topics g√©n√©raux\ngeneral_topics = analyze_topics_by_frequency(df_analysis['cleaned_text'], \"G√©n√©ral\")\nprint(\"   Topics dominants g√©n√©raux:\")\nfor i, (word, freq) in enumerate(general_topics[:10], 1):\n    print(f\"      {i:2d}. {word:15} : {freq:4} occurrences\")\n\n# Topics par sentiment (si disponible)\nif has_sentiment:\n    positive_topics = analyze_topics_by_frequency(positive_texts, \"Positif\")\n    negative_topics = analyze_topics_by_frequency(negative_texts, \"N√©gatif\")\n    \n    print(f\"\\n   Topics dominants - Sentiments Positifs:\")\n    for i, (word, freq) in enumerate(positive_topics[:8], 1):\n        print(f\"      {i:2d}. {word:15} : {freq:4} occurrences\")\n    \n    print(f\"\\n   Topics dominants - Sentiments N√©gatifs:\")\n    for i, (word, freq) in enumerate(negative_topics[:8], 1):\n        print(f\"      {i:2d}. {word:15} : {freq:4} occurrences\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:45:15.840764Z","iopub.execute_input":"2025-11-02T15:45:15.841101Z","iopub.status.idle":"2025-11-02T15:48:06.242274Z","shell.execute_reply.started":"2025-11-02T15:45:15.841074Z","shell.execute_reply":"2025-11-02T15:48:06.241359Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# √âTAPE 5: PR√âPARATION DES DONN√âES POUR DEEP LEARNING\n# ===============================================================\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"üöÄ D√âBUT DE LA PR√âPARATION POUR DEEP LEARNING\")\nprint(\"=\"*60)\n\n# V√©rifier la disponibilit√© de GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"üéØ Device utilis√©: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:50:05.976234Z","iopub.execute_input":"2025-11-02T15:50:05.976871Z","iopub.status.idle":"2025-11-02T15:50:05.983576Z","shell.execute_reply.started":"2025-11-02T15:50:05.976842Z","shell.execute_reply":"2025-11-02T15:50:05.982557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5.1 CR√âATION DU VOCABULAIRE\n# ===============================================================\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"5.1 CR√âATION DU VOCABULAIRE\")\nprint(\"=\"*50)\n\nclass Vocabulary:\n    \"\"\"Classe pour g√©rer le vocabulaire des tweets\"\"\"\n    \n    def __init__(self, freq_threshold=2):\n        self.freq_threshold = freq_threshold\n        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n        self.vocab_size = 4\n        \n    def __len__(self):\n        return self.vocab_size\n    \n    def build_vocabulary(self, sentences):\n        \"\"\"Construit le vocabulaire √† partir des phrases\"\"\"\n        print(\"üî® Construction du vocabulaire...\")\n        \n        # Compter la fr√©quence de tous les mots\n        word_freq = Counter()\n        for sentence in sentences:\n            for word in sentence.split():\n                word_freq[word] += 1\n        \n        # Ajouter les mots qui d√©passent le seuil de fr√©quence\n        for word, freq in word_freq.items():\n            if freq >= self.freq_threshold and word not in self.stoi:\n                self.stoi[word] = self.vocab_size\n                self.itos[self.vocab_size] = word\n                self.vocab_size += 1\n        \n        print(f\"   - Mots uniques trouv√©s: {len(word_freq):,}\")\n        print(f\"   - Mots ajout√©s au vocabulaire: {self.vocab_size - 4:,}\")\n        print(f\"   - Taille finale du vocabulaire: {self.vocab_size:,}\")\n        \n        # Analyser les mots rares\n        rare_words = [word for word, freq in word_freq.items() if freq < self.freq_threshold]\n        print(f\"   - Mots rares exclus (<{self.freq_threshold} occ): {len(rare_words):,}\")\n        \n        if len(rare_words) > 0:\n            print(f\"   - Exemples de mots rares: {', '.join(rare_words[:10])}\")\n    \n    def numericalize(self, text):\n        \"\"\"Convertit un texte en s√©quence num√©rique\"\"\"\n        tokens = text.split()\n        numericalized = [self.stoi.get(token, self.stoi[\"<UNK>\"]) for token in tokens]\n        return numericalized\n\n# Pr√©parer les donn√©es pour la construction du vocabulaire\nprint(\"üìù Pr√©paration des textes pour le vocabulaire...\")\nall_texts = df_analysis['cleaned_text'].tolist()\n\n# Construire le vocabulaire\nvocab = Vocabulary(freq_threshold=2)\nvocab.build_vocabulary(all_texts)\n\n# Analyser la distribution des longueurs pour d√©finir la longueur maximale\nsequence_lengths = [len(text.split()) for text in all_texts]\nprint(f\"\\nüìè ANALYSE DES LONGUEURS DE S√âQUENCES:\")\nprint(f\"   - Longueur moyenne: {np.mean(sequence_lengths):.1f} mots\")\nprint(f\"   - Longueur m√©diane: {np.median(sequence_lengths):.1f} mots\")\nprint(f\"   - Longueur max: {np.max(sequence_lengths)} mots\")\nprint(f\"   - Longueur min: {np.min(sequence_lengths)} mots\")\nprint(f\"   - 95e percentile: {np.percentile(sequence_lengths, 95):.1f} mots\")\n\n# D√©finir la longueur maximale bas√©e sur le 95e percentile\nmax_length = int(np.percentile(sequence_lengths, 95))\nprint(f\"üéØ Longueur maximale choisie: {max_length} mots (95e percentile)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:50:29.977923Z","iopub.execute_input":"2025-11-02T15:50:29.978243Z","iopub.status.idle":"2025-11-02T15:50:39.020714Z","shell.execute_reply.started":"2025-11-02T15:50:29.978219Z","shell.execute_reply":"2025-11-02T15:50:39.019527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5.2 S√âQUENCEMENT DES DONN√âES\n# ===============================================================\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"5.2 S√âQUENCEMENT DES DONN√âES\")\nprint(\"=\"*50)\n\ndef preprocess_sequences(texts, vocab, max_length):\n    \"\"\"Convertit les textes en s√©quences num√©riques avec padding\"\"\"\n    sequences = []\n    \n    for text in texts:\n        # Convertir en s√©quence num√©rique\n        numericalized = vocab.numericalize(text)\n        \n        # Tronquer si n√©cessaire\n        if len(numericalized) > max_length:\n            numericalized = numericalized[:max_length]\n        \n        # Padding\n        padded_sequence = numericalized + [vocab.stoi[\"<PAD>\"]] * (max_length - len(numericalized))\n        sequences.append(padded_sequence)\n    \n    return np.array(sequences)\n\n# Pr√©parer les features (X) et labels (y)\nprint(\"üîÑ Conversion des textes en s√©quences num√©riques...\")\n\nX_sequences = preprocess_sequences(all_texts, vocab, max_length)\n\nif has_sentiment:\n    # Convertir les labels sentiment (0=n√©gatif, 4=positif ‚Üí 0,1)\n    y = df_analysis['target'].apply(lambda x: 0 if x == 0 else 1).values\n    print(f\"üéØ Labels de sentiment pr√©par√©s: {len(y)} √©chantillons\")\n    print(f\"   - N√©gatif (0): {np.sum(y == 0)}\")\n    print(f\"   - Positif (1): {np.sum(y == 1)}\")\nelse:\n    y = None\n    print(\"‚ö†Ô∏è  Aucun label de sentiment disponible - mode non supervis√©\")\n\nprint(f\"üì¶ Forme des s√©quences: {X_sequences.shape}\")\nprint(f\"   - Nombre d'√©chantillons: {X_sequences.shape[0]}\")\nprint(f\"   - Longueur des s√©quences: {X_sequences.shape[1]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:51:17.964094Z","iopub.execute_input":"2025-11-02T15:51:17.964437Z","iopub.status.idle":"2025-11-02T15:51:34.883096Z","shell.execute_reply.started":"2025-11-02T15:51:17.964412Z","shell.execute_reply":"2025-11-02T15:51:34.881815Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5.3 SPLIT DES DONN√âES\n# ===============================================================\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"5.3 SPLIT DES DONN√âES\")\nprint(\"=\"*50)\n\n# Dataset personnalis√© pour PyTorch\nclass TweetDataset(Dataset):\n    def __init__(self, sequences, labels=None):\n        self.sequences = torch.LongTensor(sequences)\n        self.labels = torch.LongTensor(labels) if labels is not None else None\n    \n    def __len__(self):\n        return len(self.sequences)\n    \n    def __getitem__(self, idx):\n        if self.labels is not None:\n            return self.sequences[idx], self.labels[idx]\n        else:\n            return self.sequences[idx]\n\n# Division des donn√©es selon la disponibilit√© des labels\nif has_sentiment and y is not None:\n    print(\"üéØ DIVISION AVEC LABELS (APPRENTISSAGE SUPERVIS√â)\")\n    \n    # Split stratifi√© pour maintenir la distribution des sentiments\n    X_temp, X_test, y_temp, y_test = train_test_split(\n        X_sequences, y, test_size=0.15, random_state=42, stratify=y\n    )\n    \n    X_train, X_val, y_train, y_val = train_test_split(\n        X_temp, y_temp, test_size=0.15, random_state=42, stratify=y_temp\n    )\n    \n    print(f\"üìä R√âPARTITION DES DONN√âES:\")\n    print(f\"   - Train: {len(X_train):,} √©chantillons ({len(X_train)/len(X_sequences)*100:.1f}%)\")\n    print(f\"   - Validation: {len(X_val):,} √©chantillons ({len(X_val)/len(X_sequences)*100:.1f}%)\")\n    print(f\"   - Test: {len(X_test):,} √©chantillons ({len(X_test)/len(X_sequences)*100:.1f}%)\")\n    \n    # V√©rifier la distribution des sentiments dans chaque split\n    print(f\"\\nüé≠ DISTRIBUTION DES SENTIMENTS PAR SPLIT:\")\n    \n    for split_name, split_y in [(\"Train\", y_train), (\"Validation\", y_val), (\"Test\", y_test)]:\n        neg_count = np.sum(split_y == 0)\n        pos_count = np.sum(split_y == 1)\n        total = len(split_y)\n        print(f\"   {split_name:12}: {neg_count:4} n√©gatifs ({neg_count/total*100:.1f}%) | \"\n              f\"{pos_count:4} positifs ({pos_count/total*100:.1f}%)\")\n    \n    # Cr√©er les datasets PyTorch\n    train_dataset = TweetDataset(X_train, y_train)\n    val_dataset = TweetDataset(X_val, y_val)\n    test_dataset = TweetDataset(X_test, y_test)\n    \nelse:\n    print(\"üîç MODE NON SUPERVIS√â - Donn√©es sans labels\")\n    # Utiliser tout le dataset pour l'analyse non supervis√©e\n    full_dataset = TweetDataset(X_sequences)\n    \n    # Pour l'√©valuation, on peut quand m√™me splitter si on veut\n    indices = np.arange(len(X_sequences))\n    train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\n    \n    X_train = X_sequences[train_idx]\n    X_test = X_sequences[test_idx]\n    \n    train_dataset = TweetDataset(X_train)\n    test_dataset = TweetDataset(X_test)\n    val_dataset = None\n    \n    print(f\"üìä R√âPARTITION DES DONN√âES:\")\n    print(f\"   - Train: {len(X_train):,} √©chantillons\")\n    print(f\"   - Test: {len(X_test):,} √©chantillons\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:52:22.580515Z","iopub.execute_input":"2025-11-02T15:52:22.580821Z","iopub.status.idle":"2025-11-02T15:52:24.193264Z","shell.execute_reply.started":"2025-11-02T15:52:22.580797Z","shell.execute_reply":"2025-11-02T15:52:24.192238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# PR√âPARATION DES DATALOADERS\n# ===============================================================\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"PR√âPARATION DES DATALOADERS\")\nprint(\"=\"*50)\n\n# D√©finir les param√®tres des DataLoaders\nbatch_size = 64\nprint(f\"üîß Configuration des DataLoaders:\")\nprint(f\"   - Batch size: {batch_size}\")\nprint(f\"   - Device: {device}\")\n\nif has_sentiment:\n    # DataLoaders pour l'apprentissage supervis√©\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n    \n    print(f\"‚úÖ DataLoaders cr√©√©s:\")\n    print(f\"   - Train: {len(train_loader)} batches\")\n    print(f\"   - Validation: {len(val_loader)} batches\")\n    print(f\"   - Test: {len(test_loader)} batches\")\n    \nelse:\n    # DataLoaders pour l'analyse non supervis√©e\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n    \n    print(f\"‚úÖ DataLoaders cr√©√©s (non supervis√©):\")\n    print(f\"   - Train: {len(train_loader)} batches\")\n    print(f\"   - Test: {len(test_loader)} batches\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:52:48.045396Z","iopub.execute_input":"2025-11-02T15:52:48.045718Z","iopub.status.idle":"2025-11-02T15:52:48.057492Z","shell.execute_reply.started":"2025-11-02T15:52:48.045695Z","shell.execute_reply":"2025-11-02T15:52:48.056386Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# V√âRIFICATION ET TESTS\n# ===============================================================\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"V√âRIFICATION ET TESTS\")\nprint(\"=\"*50)\n\n# Tester un batch\nprint(\"üß™ Test d'un batch d'entra√Ænement...\")\n\nif has_sentiment:\n    # Mode supervis√©\n    data_iter = iter(train_loader)\n    sequences_batch, labels_batch = next(data_iter)\n    \n    print(f\"‚úÖ Batch test√© avec succ√®s:\")\n    print(f\"   - Forme des s√©quences: {sequences_batch.shape}\")\n    print(f\"   - Forme des labels: {labels_batch.shape}\")\n    print(f\"   - Device: {sequences_batch.device}\")\n    \n    # Afficher un exemple\n    print(f\"\\nüîç EXEMPLE D'UN √âCHANTILLON:\")\n    sample_idx = 0\n    sample_sequence = sequences_batch[sample_idx].cpu().numpy()\n    sample_label = labels_batch[sample_idx].cpu().numpy()\n    \n    # Convertir la s√©quence num√©rique back en texte\n    original_text = []\n    for token_id in sample_sequence:\n        if token_id == vocab.stoi[\"<PAD>\"]:\n            break\n        original_text.append(vocab.itos.get(token_id, \"<UNK>\"))\n    \n    print(f\"   - S√©quence num√©rique: {sample_sequence[:10]}...\")\n    print(f\"   - Texte reconstruit: {' '.join(original_text[:10])}...\")\n    print(f\"   - Label: {'Positif' if sample_label == 1 else 'N√©gatif'}\")\n    \nelse:\n    # Mode non supervis√©\n    data_iter = iter(train_loader)\n    sequences_batch = next(data_iter)\n    \n    print(f\"‚úÖ Batch test√© avec succ√®s (non supervis√©):\")\n    print(f\"   - Forme des s√©quences: {sequences_batch.shape}\")\n    \n    # Afficher un exemple\n    print(f\"\\nüîç EXEMPLE D'UN √âCHANTILLON:\")\n    sample_idx = 0\n    sample_sequence = sequences_batch[sample_idx].cpu().numpy()\n    \n    # Convertir la s√©quence num√©rique back en texte\n    original_text = []\n    for token_id in sample_sequence:\n        if token_id == vocab.stoi[\"<PAD>\"]:\n            break\n        original_text.append(vocab.itos.get(token_id, \"<UNK>\"))\n    \n    print(f\"   - S√©quence num√©rique: {sample_sequence[:10]}...\")\n    print(f\"   - Texte reconstruit: {' '.join(original_text[:10])}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:53:08.926941Z","iopub.execute_input":"2025-11-02T15:53:08.927656Z","iopub.status.idle":"2025-11-02T15:53:09.714658Z","shell.execute_reply.started":"2025-11-02T15:53:08.927628Z","shell.execute_reply":"2025-11-02T15:53:09.712284Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SAUVEGARDE DES OBJETS IMPORTANTS\n# ===============================================================\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"SAUVEGARDE DES OBJETS\")\nprint(\"=\"*50)\n\nimport pickle\nimport os\n\n# Cr√©er le dossier de sauvegarde\nos.makedirs('/kaggle/working/model_assets', exist_ok=True)\n\n# Sauvegarder les objets importants\ntry:\n    # Sauvegarder le vocabulaire\n    with open('/kaggle/working/model_assets/vocabulary.pkl', 'wb') as f:\n        pickle.dump(vocab, f)\n    \n    # Sauvegarder les param√®tres\n    config = {\n        'vocab_size': vocab.vocab_size,\n        'max_length': max_length,\n        'batch_size': batch_size,\n        'has_sentiment': has_sentiment\n    }\n    \n    with open('/kaggle/working/model_assets/training_config.pkl', 'wb') as f:\n        pickle.dump(config, f)\n    \n    print(\"üíæ OBJETS SAUVEGARD√âS:\")\n    print(f\"   - Vocabulaire: /kaggle/working/model_assets/vocabulary.pkl\")\n    print(f\"   - Configuration: /kaggle/working/model_assets/training_config.pkl\")\n    print(f\"   - Taille vocabulaire: {vocab.vocab_size:,}\")\n    print(f\"   - Longueur max: {max_length}\")\n    \nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  Erreur lors de la sauvegarde: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:53:35.584916Z","iopub.execute_input":"2025-11-02T15:53:35.585526Z","iopub.status.idle":"2025-11-02T15:53:35.806328Z","shell.execute_reply.started":"2025-11-02T15:53:35.585483Z","shell.execute_reply":"2025-11-02T15:53:35.805431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# √âTAPE 6: CONFIGURATION DES EMBEDDINGS\n# ===============================================================\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport requests\nimport os\nfrom pathlib import Path\n\nprint(\"üöÄ D√âBUT DE LA CONFIGURATION DES EMBEDDINGS\")\nprint(\"=\"*60)\n\n# V√©rifier l'√©tat actuel de VOS donn√©es\nprint(\"üìä √âTAT DE VOS DONN√âES:\")\nprint(f\"   - Taille du vocabulaire: {vocab.vocab_size:,} mots\")\nprint(f\"   - Longueur des s√©quences: {max_length} tokens\")\nprint(f\"   - Device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:58:17.864812Z","iopub.execute_input":"2025-11-02T15:58:17.865235Z","iopub.status.idle":"2025-11-02T15:58:17.873277Z","shell.execute_reply.started":"2025-11-02T15:58:17.865208Z","shell.execute_reply":"2025-11-02T15:58:17.872134Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6.1 CHOIX DES EMBEDDINGS\n# ===============================================================\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"6.1 CHOIX DES EMBEDDINGS\")\nprint(\"=\"*50)\n\nclass EmbeddingManager:\n    \"\"\"G√®re les diff√©rentes strat√©gies d'embedding\"\"\"\n    \n    def __init__(self, vocab, embedding_dim=100):\n        self.vocab = vocab\n        self.embedding_dim = embedding_dim\n        self.embedding_matrix = None\n        \n    def load_glove_embeddings(self, glove_path=None):\n        \"\"\"Charge les embeddings GloVe pr√©-entra√Æn√©s\"\"\"\n        print(\"üîç Recherche d'embeddings GloVe...\")\n        \n        # Chemins possibles pour GloVe dans Kaggle\n        possible_paths = [\n            '/kaggle/input/glove6b/glove.6B.100d.txt',\n            '/kaggle/input/glove-global-vectors-for-word-representation/glove.6B.100d.txt',\n            '/kaggle/input/glove-twitter/glove.twitter.27B.100d.txt',\n            glove_path\n        ]\n        \n        glove_file = None\n        for path in possible_paths:\n            if path and os.path.exists(path):\n                glove_file = path\n                break\n                \n        if glove_file:\n            print(f\"‚úÖ Fichier GloVe trouv√©: {glove_file}\")\n            return self._load_glove_from_file(glove_file)\n        else:\n            print(\"‚ùå Aucun fichier GloVe trouv√© - initialisation al√©atoire\")\n            return None\n    \n    def _load_glove_from_file(self, glove_path):\n        \"\"\"Charge les embeddings GloVe depuis un fichier\"\"\"\n        print(f\"üìñ Chargement des embeddings GloVe depuis {glove_path}...\")\n        \n        embeddings_index = {}\n        with open(glove_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                values = line.split()\n                word = values[0]\n                coefs = np.asarray(values[1:], dtype='float32')\n                embeddings_index[word] = coefs\n        \n        print(f\"   - Embeddings charg√©s: {len(embeddings_index):,} mots\")\n        \n        # Cr√©er la matrice d'embedding pour notre vocabulaire\n        embedding_matrix = np.zeros((self.vocab.vocab_size, self.embedding_dim))\n        matched_words = 0\n        \n        for word, idx in self.vocab.stoi.items():\n            if word in embeddings_index:\n                embedding_matrix[idx] = embeddings_index[word]\n                matched_words += 1\n            elif word.lower() in embeddings_index:\n                embedding_matrix[idx] = embeddings_index[word.lower()]\n                matched_words += 1\n        \n        coverage = matched_words / self.vocab.vocab_size * 100\n        print(f\"‚úÖ Couverture du vocabulaire: {matched_words}/{self.vocab.vocab_size} ({coverage:.2f}%)\")\n        \n        return embedding_matrix\n    \n    def load_fasttext_embeddings(self):\n        \"\"\"Tente de charger des embeddings FastText\"\"\"\n        print(\"üîç Recherche d'embeddings FastText...\")\n        \n        # Chemins possibles pour FastText dans Kaggle\n        possible_paths = [\n            '/kaggle/input/fasttext-wikinews/wiki-news-300d-1M.vec',\n            '/kaggle/input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n        ]\n        \n        for path in possible_paths:\n            if os.path.exists(path):\n                print(f\"‚úÖ Fichier FastText trouv√©: {path}\")\n                return self._load_fasttext_from_file(path)\n        \n        print(\"‚ùå Aucun fichier FastText trouv√©\")\n        return None\n    \n    def _load_fasttext_from_file(self, fasttext_path):\n        \"\"\"Charge les embeddings FastText depuis un fichier\"\"\"\n        print(f\"üìñ Chargement des embeddings FastText...\")\n        \n        embeddings_index = {}\n        with open(fasttext_path, 'r', encoding='utf-8') as f:\n            # La premi√®re ligne contient le nombre de mots et la dimension\n            first_line = f.readline()\n            for line in f:\n                values = line.rstrip().split(' ')\n                word = values[0]\n                coefs = np.asarray(values[1:], dtype='float32')\n                embeddings_index[word] = coefs\n        \n        print(f\"   - Embeddings charg√©s: {len(embeddings_index):,} mots\")\n        \n        # Mettre √† jour la dimension d'embedding\n        if embeddings_index:\n            sample_embedding = next(iter(embeddings_index.values()))\n            self.embedding_dim = len(sample_embedding)\n            print(f\"   - Dimension d'embedding: {self.embedding_dim}\")\n        \n        # Cr√©er la matrice d'embedding\n        embedding_matrix = np.zeros((self.vocab.vocab_size, self.embedding_dim))\n        matched_words = 0\n        \n        for word, idx in self.vocab.stoi.items():\n            if word in embeddings_index:\n                embedding_matrix[idx] = embeddings_index[word]\n                matched_words += 1\n            elif word.lower() in embeddings_index:\n                embedding_matrix[idx] = embeddings_index[word.lower()]\n                matched_words += 1\n        \n        coverage = matched_words / self.vocab.vocab_size * 100\n        print(f\"‚úÖ Couverture du vocabulaire: {matched_words}/{self.vocab.vocab_size} ({coverage:.2f}%)\")\n        \n        return embedding_matrix\n    \n    def create_twitter_specific_embeddings(self):\n        \"\"\"Cr√©e des embeddings sp√©cifiques pour Twitter\"\"\"\n        print(\"üê¶ Cr√©ation d'embeddings sp√©cifiques Twitter...\")\n        \n        # Initialisation avec des valeurs adapt√©es au langage Twitter\n        embedding_matrix = np.random.normal(\n            scale=0.1, \n            size=(self.vocab.vocab_size, self.embedding_dim)\n        )\n        \n        # Initialisation sp√©ciale pour les tokens Twitter courants\n        twitter_words = {\n            'rt': 0.5, 'lol': 0.8, 'omg': 0.7, 'haha': 0.6, \n            'love': 0.9, 'happy': 0.8, 'sad': -0.8, 'angry': -0.9,\n            'good': 0.7, 'bad': -0.7, 'great': 0.9, 'terrible': -0.9\n        }\n        \n        for word, sentiment_bias in twitter_words.items():\n            if word in self.vocab.stoi:\n                idx = self.vocab.stoi[word]\n                # Initialiser avec un biais de sentiment\n                embedding_matrix[idx] += sentiment_bias * 0.1\n        \n        print(f\"‚úÖ Embeddings Twitter cr√©√©s: {len(twitter_words)} mots initialis√©s avec biais de sentiment\")\n        return embedding_matrix\n    \n    def initialize_random_embeddings(self):\n        \"\"\"Initialisation al√©atoire des embeddings\"\"\"\n        print(\"üé≤ Initialisation al√©atoire des embeddings...\")\n        \n        # Initialisation Xavier/Glorot pour une meilleure convergence\n        scale = np.sqrt(2.0 / (self.vocab.vocab_size + self.embedding_dim))\n        embedding_matrix = np.random.normal(\n            scale=scale, \n            size=(self.vocab.vocab_size, self.embedding_dim)\n        )\n        \n        # Initialisation √† z√©ro pour les tokens de padding\n        if '<PAD>' in self.vocab.stoi:\n            pad_idx = self.vocab.stoi['<PAD>']\n            embedding_matrix[pad_idx] = np.zeros(self.embedding_dim)\n        \n        print(f\"‚úÖ Embeddings al√©atoires initialis√©s (Xavier)\")\n        return embedding_matrix\n\n# Tester diff√©rentes strat√©gies d'embedding\nprint(\"üß™ TEST DES DIFF√âRENTES STRAT√âGIES D'EMBEDDING...\")\n\nembedding_manager = EmbeddingManager(vocab, embedding_dim=100)\n\n# 1. Essayer GloVe en premier\nglove_embeddings = embedding_manager.load_glove_embeddings()\n\nif glove_embeddings is not None:\n    embedding_manager.embedding_matrix = glove_embeddings\n    strategy = \"GloVe pr√©-entra√Æn√©s\"\n    print(\"üéØ STRAT√âGIE CHOISIE: GloVe pr√©-entra√Æn√©s\")\n\nelse:\n    # 2. Essayer FastText\n    fasttext_embeddings = embedding_manager.load_fasttext_embeddings()\n    \n    if fasttext_embeddings is not None:\n        embedding_manager.embedding_matrix = fasttext_embeddings\n        strategy = \"FastText pr√©-entra√Æn√©s\"\n        print(\"üéØ STRAT√âGIE CHOISIE: FastText pr√©-entra√Æn√©s\")\n    \n    else:\n        # 3. Embeddings sp√©cifiques Twitter\n        twitter_embeddings = embedding_manager.create_twitter_specific_embeddings()\n        embedding_manager.embedding_matrix = twitter_embeddings\n        strategy = \"Embeddings sp√©cifiques Twitter\"\n        print(\"üéØ STRAT√âGIE CHOISIE: Embeddings sp√©cifiques Twitter\")\n\nprint(f\"‚úÖ Matrice d'embedding cr√©√©e: {embedding_manager.embedding_matrix.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T15:58:40.988287Z","iopub.execute_input":"2025-11-02T15:58:40.989181Z","iopub.status.idle":"2025-11-02T15:58:41.48932Z","shell.execute_reply.started":"2025-11-02T15:58:40.989149Z","shell.execute_reply":"2025-11-02T15:58:41.488331Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6.2 ADAPTATION DES EMBEDDINGS\n# ===============================================================\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"6.2 ADAPTATION DES EMBEDDINGS\")\nprint(\"=\"*50)\n\nclass AdaptiveEmbeddingLayer(nn.Module):\n    \"\"\"Couche d'embedding adaptative avec diff√©rentes strat√©gies\"\"\"\n    \n    def __init__(self, vocab_size, embedding_dim, embedding_matrix=None, \n                 trainable=True, dropout=0.1):\n        super(AdaptiveEmbeddingLayer, self).__init__()\n        \n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.trainable = trainable\n        \n        # Cr√©er la couche d'embedding\n        if embedding_matrix is not None:\n            self.embedding = nn.Embedding.from_pretrained(\n                torch.FloatTensor(embedding_matrix),\n                freeze=not trainable,\n                padding_idx=0  # <PAD> token\n            )\n            print(f\"‚úÖ Embeddings pr√©-entra√Æn√©s charg√©s (trainable: {trainable})\")\n        else:\n            self.embedding = nn.Embedding(\n                vocab_size, \n                embedding_dim, \n                padding_idx=0\n            )\n            # Initialisation Xavier\n            nn.init.xavier_uniform_(self.embedding.weight)\n            print(f\"‚úÖ Embeddings al√©atoires initialis√©s (trainable: {trainable})\")\n        \n        # Dropout pour r√©gularisation\n        self.dropout = nn.Dropout(dropout)\n        \n        # Couche de projection pour adapter la dimension si n√©cessaire\n        self.projection = None\n        if embedding_dim != 100:  # Dimension cible pour le mod√®le\n            self.projection = nn.Linear(embedding_dim, 100)\n            print(f\"‚úÖ Couche de projection ajout√©e: {embedding_dim} ‚Üí 100\")\n    \n    def forward(self, x):\n        embeddings = self.embedding(x)\n        \n        if self.projection is not None:\n            embeddings = self.projection(embeddings)\n        \n        return self.dropout(embeddings)\n\n# Test des diff√©rentes strat√©gies d'adaptation\nprint(\"üß™ CONFIGURATION DES STRAT√âGIES D'ADAPTATION...\")\n\n# Strat√©gie 1: Fine-tuning complet\nprint(\"\\n1. üîÑ FINE-TUNING COMPLET:\")\nembedding_finetune = AdaptiveEmbeddingLayer(\n    vocab_size=vocab.vocab_size,\n    embedding_dim=embedding_manager.embedding_dim,\n    embedding_matrix=embedding_manager.embedding_matrix,\n    trainable=True,  # Les embeddings seront mis √† jour\n    dropout=0.2\n).to(device)\n\nprint(f\"   - Embeddings trainables: OUI\")\nprint(f\"   - Dropout: 0.2\")\nprint(f\"   - Param√®tres √† entra√Æner: {sum(p.numel() for p in embedding_finetune.parameters()):,}\")\n\n# Strat√©gie 2: Embeddings fig√©s\nprint(\"\\n2. üßä EMBEDDINGS FIG√âS:\")\nembedding_frozen = AdaptiveEmbeddingLayer(\n    vocab_size=vocab.vocab_size,\n    embedding_dim=embedding_manager.embedding_dim,\n    embedding_matrix=embedding_manager.embedding_matrix,\n    trainable=False,  # Les embeddings restent fixes\n    dropout=0.1\n).to(device)\n\nprint(f\"   - Embeddings trainables: NON\")\nprint(f\"   - Dropout: 0.1\")\nprint(f\"   - Param√®tres √† entra√Æner: {sum(p.numel() for p in embedding_frozen.parameters() if p.requires_grad):,}\")\n\n# Strat√©gie 3: Apprentissage from scratch\nprint(\"\\n3. üé≤ FROM SCRATCH:\")\nembedding_scratch = AdaptiveEmbeddingLayer(\n    vocab_size=vocab.vocab_size,\n    embedding_dim=100,  # Dimension standard\n    embedding_matrix=None,  # Pas d'embedding pr√©-entra√Æn√©\n    trainable=True,\n    dropout=0.3\n).to(device)\n\nprint(f\"   - Embeddings trainables: OUI\")\nprint(f\"   - Dimension: 100\")\nprint(f\"   - Dropout: 0.3\")\nprint(f\"   - Param√®tres √† entra√Æner: {sum(p.numel() for p in embedding_scratch.parameters()):,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T16:08:07.157183Z","iopub.execute_input":"2025-11-02T16:08:07.158716Z","iopub.status.idle":"2025-11-02T16:08:07.620568Z","shell.execute_reply.started":"2025-11-02T16:08:07.158671Z","shell.execute_reply":"2025-11-02T16:08:07.619444Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# COMBINAISON D'EMBEDDINGS\n# ===============================================================\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"COMBINAISON D'EMBEDDINGS\")\nprint(\"=\"*50)\n\nclass MultiEmbeddingLayer(nn.Module):\n    \"\"\"Combine plusieurs types d'embeddings\"\"\"\n    \n    def __init__(self, vocab_size, embedding_configs):\n        super(MultiEmbeddingLayer, self).__init__()\n        \n        self.embedding_layers = nn.ModuleList()\n        self.output_dim = 0\n        \n        for config in embedding_configs:\n            layer = AdaptiveEmbeddingLayer(\n                vocab_size=vocab_size,\n                embedding_dim=config['dim'],\n                embedding_matrix=config.get('matrix'),\n                trainable=config.get('trainable', True),\n                dropout=config.get('dropout', 0.1)\n            )\n            self.embedding_layers.append(layer)\n            self.output_dim += config['dim']\n        \n        # Couche de combinaison\n        self.combination_layer = nn.Linear(self.output_dim, 100)\n        self.layer_norm = nn.LayerNorm(100)\n        self.dropout = nn.Dropout(0.2)\n        \n        print(f\"‚úÖ Combinaison de {len(embedding_configs)} embeddings\")\n        print(f\"   - Dimension totale: {self.output_dim} ‚Üí 100\")\n    \n    def forward(self, x):\n        embeddings = []\n        for layer in self.embedding_layers:\n            emb = layer(x)\n            embeddings.append(emb)\n        \n        # Concatenation des embeddings\n        combined = torch.cat(embeddings, dim=-1)\n        \n        # Projection vers dimension cible\n        output = self.combination_layer(combined)\n        output = self.layer_norm(output)\n        output = self.dropout(output)\n        \n        return output\n\n# Configuration pour la combinaison d'embeddings\nprint(\"üîÑ CONFIGURATION DE LA COMBINAISON D'EMBEDDINGS...\")\n\nembedding_configs = [\n    {\n        'name': 'GloVe_Twitter',\n        'dim': embedding_manager.embedding_dim,\n        'matrix': embedding_manager.embedding_matrix,\n        'trainable': True,\n        'dropout': 0.1\n    },\n    {\n        'name': 'Character_Level',\n        'dim': 50,\n        'matrix': None,  # Appris from scratch\n        'trainable': True,\n        'dropout': 0.2\n    }\n]\n\nmulti_embedding = MultiEmbeddingLayer(\n    vocab_size=vocab.vocab_size,\n    embedding_configs=embedding_configs\n).to(device)\n\nprint(f\"‚úÖ Couche multi-embeddings cr√©√©e\")\nprint(f\"   - Dimension de sortie: {multi_embedding.output_dim}\")\nprint(f\"   - Param√®tres totaux: {sum(p.numel() for p in multi_embedding.parameters()):,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T16:08:48.73395Z","iopub.execute_input":"2025-11-02T16:08:48.734397Z","iopub.status.idle":"2025-11-02T16:08:49.024397Z","shell.execute_reply.started":"2025-11-02T16:08:48.734364Z","shell.execute_reply":"2025-11-02T16:08:49.022906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================================================\n# TESTS ET VALIDATION DES COUCHES D'EMBEDDINGS\n# ===============================================================\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"üîé TESTS ET VALIDATION DES COUCHES D'EMBEDDINGS\")\nprint(\"=\"*60)\n\n# ---------------------------------------------------------------\n# 1Ô∏è‚É£ Param√®tres de base\n# ---------------------------------------------------------------\nvocab_size = 10000      # Taille du vocabulaire (√† adapter selon ton tokenizer)\nembedding_dim = 100      # Dimension de base des embeddings\nbatch_size = 32\nseq_len = 50\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# ---------------------------------------------------------------\n# 2Ô∏è‚É£ D√©finition des diff√©rentes strat√©gies d'embeddings\n# ---------------------------------------------------------------\n\n# a. Embedding fine-tunable (classique, entra√Æn√©)\nembedding_finetune = nn.Embedding(vocab_size, embedding_dim).to(device)\n\n# b. Embedding gel√© (pr√©-entra√Æn√© fig√©)\nembedding_frozen = nn.Embedding(vocab_size, embedding_dim).to(device)\nembedding_frozen.weight.requires_grad = False\n\n# c. Embedding entra√Æn√© from scratch\nembedding_scratch = nn.Embedding(vocab_size, embedding_dim).to(device)\n\n\n# ---------------------------------------------------------------\n# 3Ô∏è‚É£ Mod√®le multi-embedding corrig√©\n# ---------------------------------------------------------------\n\nclass MultiEmbedding(nn.Module):\n    def __init__(self, vocab_size, emb_dims, output_dim=100, dropout=0.2):\n        \"\"\"\n        Combine plusieurs embeddings en concat√©nant leurs repr√©sentations\n        puis en les projetant dans une dimension cible.\n        \"\"\"\n        super().__init__()\n        self.embeddings = nn.ModuleList([\n            nn.Embedding(vocab_size, dim) for dim in emb_dims\n        ])\n        total_dim = sum(emb_dims)  # somme des dimensions de tous les embeddings\n        self.combination_layer = nn.Linear(total_dim, output_dim)\n        self.layer_norm = nn.LayerNorm(output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # Combiner les embeddings concat√©n√©s\n        embedded = [emb(x) for emb in self.embeddings]\n        combined = torch.cat(embedded, dim=-1)\n        # Passage lin√©aire + normalisation\n        output = self.combination_layer(combined)\n        output = self.layer_norm(output)\n        output = self.dropout(output)\n        return output\n\n# Instancier le multi-embedding\nmulti_embedding = MultiEmbedding(\n    vocab_size=vocab_size,\n    emb_dims=[50, 100, 50],  # total_dim = 200\n    output_dim=embedding_dim  # sortie finale √† 100 dimensions\n).to(device)\n\n\n# ---------------------------------------------------------------\n# 4Ô∏è‚É£ Cr√©ation d‚Äôun batch de test\n# ---------------------------------------------------------------\ntest_batch = torch.randint(0, vocab_size, (batch_size, seq_len)).to(device)\nprint(f\"\\nüì¶ Batch de test g√©n√©r√© : {test_batch.shape}\")\n\n\n# ---------------------------------------------------------------\n# 5Ô∏è‚É£ Boucle de validation\n# ---------------------------------------------------------------\nstrategies = [\n    (\"Fine-tuning\", embedding_finetune),\n    (\"Embeddings fig√©s\", embedding_frozen),\n    (\"From scratch\", embedding_scratch),\n    (\"Multi-embeddings\", multi_embedding)\n]\n\nprint(\"\\nüß™ Lancement des tests...\\n\")\n\nfor name, embedding_layer in strategies:\n    with torch.no_grad():\n        output = embedding_layer(test_batch)\n        print(f\"‚úÖ {name}:\")\n        print(f\"   - Input shape : {test_batch.shape}\")\n        print(f\"   - Output shape : {output.shape}\")\n        print(f\"   - Norme moyenne : {output.norm(dim=-1).mean().item():.4f}\\n\")\n\nprint(\"üéØ Tous les embeddings ont √©t√© test√©s avec succ√®s !\")\nprint(\"=\"*60)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T16:15:21.617497Z","iopub.execute_input":"2025-11-02T16:15:21.618611Z","iopub.status.idle":"2025-11-02T16:15:21.769612Z","shell.execute_reply.started":"2025-11-02T16:15:21.618574Z","shell.execute_reply":"2025-11-02T16:15:21.768539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# SAUVEGARDE DE LA CONFIGURATION\n# ===============================================================\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"SAUVEGARDE DE LA CONFIGURATION\")\nprint(\"=\"*50)\n\n# Sauvegarder la matrice d'embedding\ntry:\n    os.makedirs('/kaggle/working/embeddings', exist_ok=True)\n    \n    # Sauvegarder la matrice d'embedding\n    np.save('/kaggle/working/embeddings/embedding_matrix.npy', \n            embedding_manager.embedding_matrix)\n    \n    # Sauvegarder la configuration\n    embedding_config = {\n        'strategy': strategy,\n        'embedding_dim': embedding_manager.embedding_dim,\n        'vocab_size': vocab.vocab_size,\n        'coverage': coverage if 'coverage' in locals() else 'N/A',\n        'trainable_params_finetune': sum(p.numel() for p in embedding_finetune.parameters()),\n        'trainable_params_frozen': sum(p.numel() for p in embedding_frozen.parameters() if p.requires_grad),\n        'trainable_params_scratch': sum(p.numel() for p in embedding_scratch.parameters()),\n        'trainable_params_multi': sum(p.numel() for p in multi_embedding.parameters())\n    }\n    \n    with open('/kaggle/working/embeddings/embedding_config.pkl', 'wb') as f:\n        pickle.dump(embedding_config, f)\n    \n    print(\"üíæ CONFIGURATION SAUVEGARD√âE:\")\n    print(f\"   - Matrice d'embedding: /kaggle/working/embeddings/embedding_matrix.npy\")\n    print(f\"   - Configuration: /kaggle/working/embeddings/embedding_config.pkl\")\n    print(f\"   - Strat√©gie: {strategy}\")\n    print(f\"   - Dimension: {embedding_manager.embedding_dim}\")\n    \nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  Erreur lors de la sauvegarde: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T16:18:34.483646Z","iopub.execute_input":"2025-11-02T16:18:34.484083Z","iopub.status.idle":"2025-11-02T16:18:34.563806Z","shell.execute_reply.started":"2025-11-02T16:18:34.484053Z","shell.execute_reply":"2025-11-02T16:18:34.562697Z"}},"outputs":[],"execution_count":null}]}